---
title: "02 Statistical Learning - Conceptual"
output: html_document
fontsize: 12pt
---

1. For each of parts (a) through (d), indicate whether we would generally
expect the performance of a flexible statistical learning method to be
better or worse than an inflexible method. Justify your answer.

(a) The sample size n is extremely large, and the number of predictors p is small.  
**A flexible method will fit the data closer given the large sample size**  

(b) The number of predictors p is extremely large, and the number
of observations n is small.  
**In this case a flexible method is likely to overfit given the small number of observations**  

(c) The relationship between the predictors and response is highly
non-linear.  
**A flexible method will fit the non-linear relationship closer than an inflexible method**  

(d) The variance of the error terms, i.e. $σ^2^ = Var()$, is extremely
high.  
**A flexible method will try to fit the error term so a more inflexible method will be better in this case**  

2. Explain whether each scenario is a classification or regression problem, and indicate whether we are most interested in inference or prediction. Finally, provide n and p.  

(a) We collect a set of data on the top 500 firms in the US. For each
firm we record profit, number of employees, industry and the
CEO salary. We are interested in understanding which factors
affect CEO salary.  
**Regression problem most interested in inference. n = 500, p = 3.**

(b) We are considering launching a new product and wish to know
whether it will be a success or a failure. We collect data on 20
similar products that were previously launched. For each product we have recorded whether it was a success or failure, price
charged for the product, marketing budget, competition price,
and ten other variables.  
**Classification problem most interested in prediction. n = 20, p = 13**  

(c) We are interested in predicting the % change in the USD/Euro
exchange rate in relation to the weekly changes in the world
stock markets. Hence we collect weekly data for all of 2012. For
each week we record the % change in the USD/Euro, the %
change in the US market, the % change in the British market,
and the % change in the German market.  
**Regression problem most interested in prediction. n = 52, p = 3**

3. We now revisit the bias-variance decomposition.

(a) Provide a sketch of typical (squared) bias, variance, training error, test error, and Bayes (or irreducible) error curves, on a single plot, as we go from less flexible statistical learning methods towards more flexible approaches. The x-axis should represent the amount of flexibility in the method, and the y-axis should represent the values for each curve. There should be five curves.
Make sure to label each one.  
(b) Explain why each of the five curves has the shape displayed in
part (a).
* **(squared) bias: decreases exponentially as flexibility increases. Less flexible methods are simpler than more flexible methods and therefore result in higher bias.**
* **variance: increases exponentially as flexibility increases. Any changes to the training data is likeley to have a larger impact on a more flexibile method.** 
* **training error: decreases as flexibility increases as a more flexible method will be a closer fit to the training dataset.**
* **test error: will decrease will flexibility to a point and will then increase as the additional flexibility overfits the training data.**
* **Bayes (irreducible) error: straight horizontal line.**

4. You will now think of some real-life applications for statistical learning.
(a) Describe three real-life applications in which classification might
be useful. Describe the response, as well as the predictors. Is the
goal of each application inference or prediction? Explain your
answer.
(b) Describe three real-life applications in which regression might
be useful. Describe the response, as well as the predictors. Is the
goal of each application inference or prediction? Explain your
answer.
(c) Describe three real-life applications in which cluster analysis
might be useful.

5. What are the advantages and disadvantages of a very flexible (versus
a less flexible) approach for regression or classification? Under what
circumstances might a more flexible approach be preferred to a less
flexible approach? When might a less flexible approach be preferred?

6. Describe the differences between a parametric and a non-parametric
statistical learning approach. What are the advantages of a parametric approach to regression or classification (as opposed to a nonparametric approach)? What are its disadvantages?

7. The table below provides a training data set containing six observations, three predictors, and one qualitative response variable.  

Obs. | X1 | X2 | X3 | Y
:---|---:|---:|---:|---
1 | 0 | 3 | 0 | Red
2 | 2 | 0 | 0 | Red
3 | 0 | 1 | 3 | Red
4 | 0 | 1 | 2 | Green
5 | −1 | 0 | 1 | Green
6 | 1 | 1 | 1 | Red

Suppose we wish to use this data set to make a prediction for Y when
X1 = X2 = X3 = 0 using K-nearest neighbors.
(a) Compute the Euclidean distance between each observation and
the test point, X1 = X2 = X3 = 0.  
**Obs1: $\sqrt{0^2 + 3^2 + 0^2} = 3$**  
**Obs2: $\sqrt{2^2 + 0^2 + 0^2} = 2$**  
**Obs3: $\sqrt{0^2 + 1^2 + 3^2} = \sqrt{10} = 3.16**  
**Obs4: $\sqrt{0^2 + 1^2 + 2^2} = \sqrt{5} = 2.24**  
**Obs5: $\sqrt{(-1)^2 + 0^2 + 1^2} = \sqrt{2} = 1.41**  
**Obs6: $\sqrt{1^2 + 1^2 + 1^2} = \sqrt{3} = 1.73**  

(b) What is our prediction with K = 1? Why?
**Green: Obs 5 (Green) is the nearest to X1 = X2 = X3 = 0.**

(c) What is our prediction with K = 3? Why?
**Red: The three nearest to X1 = X2 = X3 = 0 are Obs 5 (Green), Obs 6 (Red) and Obs 2 (Red). The estimated probabilities are 1/3 for the Green class and 2/3 for the Red class. Therefore the classification is the higher probability: Red.**

(d) If the Bayes decision boundary in this problem is highly nonlinear, then would we expect the best value for K to be large or small? Why?  
**We would expect the best value for K to be small. A smaller value of K is a more flexible method which will be a better fit to the highly nonlinear Bayes decision boundary.**


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
summary(cars)
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
